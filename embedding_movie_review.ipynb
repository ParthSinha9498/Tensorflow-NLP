{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from os import listdir\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten, Embedding\n",
    "from keras.layers import Conv1D, MaxPool1D\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    Xtrain=tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    Xtest =tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc, vocab):\n",
    "    tokens=doc.split()\n",
    "    re_punc= re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('', w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[w for w in tokens if not w in stop_words]\n",
    "    tokens=[word for word in tokens if len(word)> 1]\n",
    "    return tokens\n",
    "\n",
    "def doc_to_line(filename, vocab):\n",
    "    doc= load_doc(filename)\n",
    "    tokens=clean_doc(doc)\n",
    "    tokens=[w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens) \n",
    "\n",
    "\n",
    "def process_docs(directory,vocab, isTrain):\n",
    "    lines=[]\n",
    "    for filename in listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        if isTrain and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not isTrain and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path=directory+\"/\"+filename\n",
    "        line=doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc=load_doc(filename)\n",
    "    tokens=clean_doc(doc)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def load_clean_dataset(vocab, isTrain):\n",
    "    neg=process_docs('dataset/movie_review/neg', vocab, isTrain)\n",
    "    pos=process_docs('dataset/movie_review/pos', vocab, isTrain)\n",
    "    docs=neg+pos\n",
    "    labels= [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(vocab_size,100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # plot_model(model,to_file='model.png',show_shape=True)\n",
    "    return model\n",
    "\n",
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores=[]\n",
    "    n_repeats=30\n",
    "    n_words=Xtrain.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        model=define_model(n_words)\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "        loss, acc= model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "    return scores\n",
    "\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    encoded= tokenizer.texts_to_sequences(docs)\n",
    "    padded= pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26897\n"
     ]
    }
   ],
   "source": [
    "vocab_filename='vocab.txt'\n",
    "vocab=load_doc(vocab_filename)\n",
    "vocab=set(vocab.split())\n",
    "train_docs, y_train= load_clean_dataset(vocab, True)\n",
    "tokenizer= create_tokenizer(train_docs)\n",
    "vocab_size=len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "y_train=np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1319, 100)         2689700   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1312, 32)          25632     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 656, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 20992)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                209930    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2925273 (11.16 MB)\n",
      "Trainable params: 2925273 (11.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Parth\\anaconda3\\envs\\tf-nlp\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Parth\\anaconda3\\envs\\tf-nlp\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "57/57 - 4s - loss: 0.6892 - accuracy: 0.5283 - 4s/epoch - 62ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 2s - loss: 0.5656 - accuracy: 0.7061 - 2s/epoch - 38ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 2s - loss: 0.1801 - accuracy: 0.9367 - 2s/epoch - 38ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 2s - loss: 0.0181 - accuracy: 0.9983 - 2s/epoch - 38ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 2s - loss: 0.0028 - accuracy: 1.0000 - 2s/epoch - 38ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 2s - loss: 0.0012 - accuracy: 1.0000 - 2s/epoch - 37ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 2s - loss: 7.3522e-04 - accuracy: 1.0000 - 2s/epoch - 37ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 2s - loss: 5.0242e-04 - accuracy: 1.0000 - 2s/epoch - 36ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 2s - loss: 3.6740e-04 - accuracy: 1.0000 - 2s/epoch - 36ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 2s - loss: 2.8157e-04 - accuracy: 1.0000 - 2s/epoch - 37ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parth\\anaconda3\\envs\\tf-nlp\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "max_length= max([len(s.split()) for s in train_docs]) \n",
    "max_length\n",
    "Xtrain= encode_docs(tokenizer, max_length, train_docs)\n",
    "model= define_model(vocab_size, max_length)\n",
    "model.fit(Xtrain, np.asarray(y_train), epochs=10, verbose=2)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, y_train= load_clean_dataset(vocab, True)\n",
    "test_docs, y_test= load_clean_dataset(vocab, False)\n",
    "tokenizer=create_tokenizer(train_docs)\n",
    "vocab_size=len(tokenizer.word_index) +1 \n",
    "max_length= max([len(s.split()) for s in train_docs]) \n",
    "Xtrain= encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest= encode_docs(tokenizer, max_length, test_docs)\n",
    "y_train=np.asarray(y_train)\n",
    "y_test=np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 1.0\n",
      "Test accuracy 0.8799999952316284\n"
     ]
    }
   ],
   "source": [
    "model=load_model('model.h5')\n",
    "_, acc= model.evaluate(Xtrain, y_train, verbose=0)\n",
    "print(f\"Train accuracy {acc}\")\n",
    "_, acc= model.evaluate(Xtest, y_test, verbose=0)\n",
    "print(f\"Test accuracy {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    line= clean_doc(review, vocab)\n",
    "    padded= encode_docs(tokenizer, max_length, [line])\n",
    "    yhat=model.predict(padded, verbose=0)\n",
    "    percent_pos= yhat[0,0]\n",
    "    if round(percent_pos)==0:\n",
    "        return (1- percent_pos), \"NEGATIVE\"\n",
    "    return percent_pos, \"POSITIVE\"\n",
    "\n",
    "def clean_doc(doc, vocab):\n",
    "    tokens=doc.split()\n",
    "    re_punc= re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('', w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word in vocab]\n",
    "    tokens= ' '.join(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9917069831863046, 'NEGATIVE')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"Really bad movie\"\n",
    "predict_sentiment(text, vocab, tokenizer, max_length, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
