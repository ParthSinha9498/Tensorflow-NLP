{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from os import listdir\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten, Embedding, Input, Dropout\n",
    "from keras.layers import Conv1D, MaxPool1D\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    Xtrain=tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    Xtest =tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def clean_doc(doc, vocab):\n",
    "    tokens=doc.split()\n",
    "    re_punc= re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('', w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[w for w in tokens if not w in stop_words]\n",
    "    tokens=[word for word in tokens if len(word)> 1]\n",
    "    return tokens\n",
    "\n",
    "def doc_to_line(filename, vocab):\n",
    "    doc= load_doc(filename)\n",
    "    tokens=clean_doc(doc,vocab)\n",
    "    tokens=[w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens) \n",
    "\n",
    "\n",
    "def process_docs(directory,vocab, isTrain):\n",
    "    lines=[]\n",
    "    for filename in listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        if isTrain and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not isTrain and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        path=directory+\"/\"+filename\n",
    "        line=doc_to_line(path, vocab)\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "def add_doc_to_vocab(filename,vocab):\n",
    "    doc=load_doc(filename)\n",
    "    tokens=clean_doc(doc, vocab)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def load_clean_dataset(vocab, isTrain):\n",
    "    neg=process_docs('dataset/movie_review/neg', vocab, isTrain)\n",
    "    pos=process_docs('dataset/movie_review/pos', vocab, isTrain)\n",
    "    docs=neg+pos\n",
    "    labels= [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    inputs1= Input(shape=(max_length,))\n",
    "    embedding1= Embedding(vocab_size,100)(inputs1)\n",
    "    conv1= Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "    drop1= Dropout(0.5)(conv1)\n",
    "    pool1= MaxPool1D(pool_size=2)(drop1)\n",
    "    flat1= Flatten()(pool1)\n",
    "\n",
    "    inputs2= Input(shape=(max_length,))\n",
    "    embedding2= Embedding(vocab_size,100)(inputs2)\n",
    "    conv2= Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "    drop2= Dropout(0.5)(conv2)\n",
    "    pool2= MaxPool1D(pool_size=2)(drop2)\n",
    "    flat2= Flatten()(pool2)\n",
    "\n",
    "    inputs3= Input(shape=(max_length,))\n",
    "    embedding3= Embedding(vocab_size,100)(inputs3)\n",
    "    conv3= Conv1D(filters=32, kernel_size=6, activation='relu')(embedding3)\n",
    "    drop3= Dropout(0.5)(conv3)\n",
    "    pool3= MaxPool1D(pool_size=2)(drop3)\n",
    "    flat3= Flatten()(pool3)\n",
    "\n",
    "    merged= concatenate([flat1, flat2, flat3])\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs= Dense(1, activation='sigmoid')(dense1)\n",
    "    model= Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores=[]\n",
    "    n_repeats=30\n",
    "    n_words=Xtrain.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        model=define_model(n_words)\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "        loss, acc= model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "    return scores\n",
    "\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    encoded= tokenizer.texts_to_sequences(docs)\n",
    "    padded= pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26897\n"
     ]
    }
   ],
   "source": [
    "vocab_filename='vocab.txt'\n",
    "vocab=load_doc(vocab_filename)\n",
    "vocab=set(vocab.split())\n",
    "train_docs, y_train= load_clean_dataset(vocab, True)\n",
    "tokenizer= create_tokenizer(train_docs)\n",
    "vocab_size=len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "y_train=np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Parth\\anaconda3\\envs\\tf-nlp\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)       [(None, 1319)]               0         []                            \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)       [(None, 1319)]               0         []                            \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)       [(None, 1319)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)     (None, 1319, 100)            2689700   ['input_10[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)    (None, 1319, 100)            2689700   ['input_11[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)    (None, 1319, 100)            2689700   ['input_12[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 1316, 32)             12832     ['embedding_9[0][0]']         \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 1314, 32)             19232     ['embedding_10[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 1314, 32)             19232     ['embedding_11[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 1316, 32)             0         ['conv1d_9[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)        (None, 1314, 32)             0         ['conv1d_10[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)        (None, 1314, 32)             0         ['conv1d_11[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPoolin  (None, 658, 32)              0         ['dropout_9[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " max_pooling1d_10 (MaxPooli  (None, 657, 32)              0         ['dropout_10[0][0]']          \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " max_pooling1d_11 (MaxPooli  (None, 657, 32)              0         ['dropout_11[0][0]']          \n",
      " ng1D)                                                                                            \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)         (None, 21056)                0         ['max_pooling1d_9[0][0]']     \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)        (None, 21024)                0         ['max_pooling1d_10[0][0]']    \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)        (None, 21024)                0         ['max_pooling1d_11[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 63104)                0         ['flatten_9[0][0]',           \n",
      " )                                                                   'flatten_10[0][0]',          \n",
      "                                                                     'flatten_11[0][0]']          \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 10)                   631050    ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 1)                    11        ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 8751457 (33.38 MB)\n",
      "Trainable params: 8751457 (33.38 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Parth\\anaconda3\\envs\\tf-nlp\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Parth\\anaconda3\\envs\\tf-nlp\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "57/57 - 8s - loss: 0.6902 - accuracy: 0.5344 - 8s/epoch - 149ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 6s - loss: 0.6247 - accuracy: 0.6328 - 6s/epoch - 112ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 6s - loss: 0.4580 - accuracy: 0.8633 - 6s/epoch - 112ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 6s - loss: 0.3396 - accuracy: 0.9689 - 6s/epoch - 110ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 6s - loss: 0.2975 - accuracy: 0.9928 - 6s/epoch - 111ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 6s - loss: 0.2813 - accuracy: 0.9939 - 6s/epoch - 111ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 6s - loss: 0.2667 - accuracy: 0.9950 - 6s/epoch - 111ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 6s - loss: 0.2527 - accuracy: 0.9972 - 6s/epoch - 111ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 6s - loss: 0.2415 - accuracy: 0.9972 - 6s/epoch - 111ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 6s - loss: 0.2311 - accuracy: 0.9972 - 6s/epoch - 111ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parth\\anaconda3\\envs\\tf-nlp\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "max_length= max([len(s.split()) for s in train_docs]) \n",
    "max_length\n",
    "Xtrain= encode_docs(tokenizer, max_length, train_docs)\n",
    "model= define_model(vocab_size, max_length)\n",
    "model.fit([Xtrain, Xtrain, Xtrain], np.asarray(y_train), epochs=10, verbose=2)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, y_train= load_clean_dataset(vocab, True)\n",
    "test_docs, y_test= load_clean_dataset(vocab, False)\n",
    "tokenizer=create_tokenizer(train_docs)\n",
    "vocab_size=len(tokenizer.word_index) +1 \n",
    "max_length= max([len(s.split()) for s in train_docs]) \n",
    "Xtrain= encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest= encode_docs(tokenizer, max_length, test_docs)\n",
    "y_train=np.asarray(y_train)\n",
    "y_test=np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9972222447395325\n",
      "Test accuracy 0.8700000047683716\n"
     ]
    }
   ],
   "source": [
    "model=load_model('model.h5')\n",
    "_, acc= model.evaluate([Xtrain,Xtrain, Xtrain], y_train, verbose=0)\n",
    "print(f\"Train accuracy {acc}\")\n",
    "_, acc= model.evaluate([Xtest,Xtest,Xtest], y_test, verbose=0)\n",
    "print(f\"Test accuracy {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    line= clean_doc(review, vocab)\n",
    "    padded= encode_docs(tokenizer, max_length, [line])\n",
    "    yhat=model.predict([padded,padded,padded], verbose=0)\n",
    "    percent_pos= yhat[0,0]\n",
    "    if round(percent_pos)==0:\n",
    "        return (1- percent_pos), \"NEGATIVE\"\n",
    "    return percent_pos, \"POSITIVE\"\n",
    "\n",
    "def clean_doc(doc, vocab):\n",
    "    tokens=doc.split()\n",
    "    re_punc= re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('', w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word in vocab]\n",
    "    tokens= ' '.join(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6401509642601013, 'NEGATIVE')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"Really bad movie\"\n",
    "predict_sentiment(text, vocab, tokenizer, max_length, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
